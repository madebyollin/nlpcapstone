**TODO, Apr 18th:**

- [x] blog post
      - [x] [Ollin] set up template
      - [x] [Irving] provide samples of data
            - [x] super high level description of process
            - [x] sample of each correctly tagged
            - [x] 1-2 samples incorrectly tagged
      - [x] [Brandon] provide samples of classifications
            - [x] Description of model architecture (might need KaTeX)
            - [x] sample of each correctly classified
            - [x] 1-2 samples incorrectly classified
      - [x] [Ollin] provide samples of audio
            - [x] 1-2 samples that sound good
            - [x] 1-2 samples that sound bad
- [x] [Ollin] Commit audio generation code

**TODO, Apr 25th:**

**Specified requirements:**

-   Complete *multiple* strawman / baseline approaches
-   record their performance
-   perform *error analysis*

- [ ] **[Irving] Data Collection:**
      - [ ] **Fixes:**
            - [ ] Support more emotion classes
            - [ ] Remove ambiguous emoji (üôÇ)
            - [ ] Any other sensible improvements to improve data quality (analyze issues in existing data/discuss w/ team, I guess?)
      - [ ] **Blog Stuff:**
            - [ ] Description of improvements made
            - [ ] Some sort of performance comparison with previous version (e.g. out of a sample 20 tweets in the previous iteration, 12 were correctly tagged, but now 18 are correctly tagged for a sample in the new iteration)
            - [ ] Error analysis: what kinds of tweets does it still tag incorrectly (if any)
- [ ] **[Brandon] Classifier:**
      - [ ] **New Model**
            - [ ] Support for more emotion classes
            - [ ] N-gram?  Decision tree?  Two-layer feed-forward network with word embeddings or some sort of featurized system?  If we have enough data for it, I think building an embedding network (in Keras if tensorflow makes it a pain) would be a good idea, just to make sure that we can actually handle the embedding stage properly in preparation for moving on to a more advanced model
      - [ ] **Blog Stuff**
            - [ ] Quick description of new model
            - [ ] Performance comparison with previous iteration (on the same dataset, in whatever way that's most convenient).  Ideally it would perform...more performantly, I guess.
            - [ ] Error analysis: what sort of tweets does the model struggle to classify (if any)?
- [ ] **[Ollin] TTS:**
      - [ ] **Fixes:**
            - [ ] Support for more emotion classes
            - [ ] Some sort of formant correction so that it doesn't sound like a chipmunk with larger pitch distortions; that is a major weakness of the current system üêø
            - [ ] Envelopes on amplitude as well as pitch (e.g. angry stuff usually emphasizes the last word)
            - [ ] Any other improvements to make it sound more natural/emotive (find reference speech in different emotions and compare spectrograms)
      - [ ] **Blog stuff**
            - [ ] Quick description of improvements
            - [ ] Comparison of TTS examples for known classes to showcase improvements
            - [ ] Error analysis: why doesn't it sound natural yet?