<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 5 : Strawman II</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
    <!-- KaTeX -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
          integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
          crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"
            integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1"
            crossorigin="anonymous">
    </script>
</head>
<body>
    <div class="post">
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Strawman II</h1>
        <p>We present an improved baseline approach for our task of emotional text-to-speech generation, and provide comparisons with our initial baseline.</p>

        <h2>Data Collection</h2>
        <p>
            Since the first strawman, we have:
            <ul>
                <li>
                    <strong>Removed the ambiguous üôÇ emoji from our emotion-emoji
                    mappings</strong>. The alternative of detecting sarcastic
                    tweets is itself an area of ongoing research and well beyond
                    the scope of our project.
                </li>
                <li>
                    <strong>Diversified the range of emotion labels</strong>. In
                    addition to joy and anger, we now also tag tweets as neutral,
                    sad, and surprised.
                </li>
                <li>
                    <strong>Expanded our corpus</strong>. Our full dataset now
                    includes just under ten thousand tweets, divided roughly evenly
                    between five emotions.
                </li>
            </ul>
        </p>
        <h3>Error Analysis</h3>
        <p>
            Error analysis for the data collection pipeline involves comparing
            the assigned labels with our own judgment. We examined ten tweets
            for each label:
            <table>
                <tr>
                    <th>Assigned Label</th>
                    <th>Agree / Disagree</th>
                </tr>
                <tr>
                    <td>Neutral</td>
                    <td>0 / 10</td>
                </tr>
                <tr>
                    <td>Joy</td>
                    <td>6 / 4</td>
                </tr>
                <tr>
                    <td>Sadness</td>
                    <td>8 / 2</td>
                </tr>
                <tr>
                    <td>Anger</td>
                    <td>10 / 0</td>
                </tr>
                <tr>
                    <td>Surprise</td>
                    <td>5 / 5</td>
                </tr>
            </table>
        </p>
        <p>
            With the trailing-emoji tagging method, angry tweets are tagged by
            far the most accurately, and sad tweets are also tagged reasonably
            well. Beyond that, however, there is considerable ambiguity and
            overlap in how emojis are used. The "neutral" emojis, in
            particular, are actually more often used in disappointed contexts:
            <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I won&#39;t be satisfied with my face until I get a nose job üòê</p>&mdash; ÿ•Ÿäÿ≤Ÿäÿ≥ üå∏‚ú® (@iizizzzz) <a href="https://twitter.com/iizizzzz/status/856334836956639235">April 24, 2017</a></blockquote>
        </p>
        <!-- TODO: <p><span class="placeholder">Comparison on specific examples</span></p> -->

        <p>We will continue to make improvements to our data pipeline, including the possible addition of further sources.</p>

        <h2>Multi-Class Classification</h2>

        <p>On this front, we have concentrated on adding an LSTM model to our pipeline, training it, and
        attempting to optimize its performance.
        We also present a refined evaluation of our first strawman model, the unigram classifier.
        </p>

        <h3>Unigram classifier</h3>
        <p>We tuned the UNK-threshold hyperparameter by a brute-force search in the range of 1 to 50.
        The dev accuracy was best with an UNK-threshold of 7 and corresponding vocabulary size of 1070 tokens.
        Here is a summary of its performance:
        </p>

        <table>
            <thead>
                <tr>
                    <th></th>
                    <th>Training</th>
                    <th>Test (dev)</th>
                </tr>
            </thead>
            <tbody>

                <tr>
                    <td>Overall accuracy</td>
                    <td>0.6776</td>
                    <td>0.3513</td>
                </tr>
                <tr>
                    <td>Mfc  accuracy</td>
                    <td>0.2180</td>
                    <td>0.1918</td>
                </tr>
            </tbody>
        </table>

        <p>Test data metrics:</p>
        <table>
            <thead>
                     <tr>
                        <th></th>
                        <th>Label 0 (NEUTRAL)</th>
                        <th>Label 1 (JOY)</th>
                        <th>Label 2 (SADNESS)</th>
                        <th>Label 3 (ANGER)</th>
                        <th>Label 4 (DISGUST)</th>
                        <th>Label 5 (SURPRISE)</th>
                    </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Precision</td>
                    <td>0.3333</td>
                    <td>0.4057</td>
                    <td>0.3084</td>
                    <td>0.3922</td>
                    <td><span class="placeholder">No data</span></td>
                    <td>0.2881</td>
                </tr>
                <tr>
                    <td>Recall</td>
                    <td>0.2804</td>
                    <td>0.4574</td>
                    <td>0.3708</td>
                    <td>0.3960</td>
                    <td><span class="placeholder">No data</span></td>
                    <td>0.2329</td>
                </tr>
                <tr>
                    <td>F1</td>
                    <td>0.3046</td>
                    <td>0.4300</td>
                    <td>0.3367</td>
                    <td>0.3941</td>
                    <td><span class="placeholder">No data</span></td>
                    <td>0.2576</td>
                </tr>
            </tbody>
        </table>

        <h3>LSTM classifier</h3>

        <p>As a second, more advanced baseline model, we implemented a character-level LSTM in TensorFlow. The input to the model is the full text of a tweet encoded as a sequence one-hot vectors. The LSTM's hidden state size is 64. The output from the final time step of the LSTM is projected into 6 dimensions (the number of labels) with a standard dense layer, and then run through a softmax to obtain a probability distribution.</p>
        <p>
        After training for about 8 hours, the model as described achieved a stunning <strong>99.99%</strong> accuracy on the training set and <strong>26.24%</strong> dev accuracy, suggesting egregious and unabashed overfitting. So, we re-trained the network with 50% dropout applied to the LSTM cell inputs and outputs. This resulted substantially improved performance, which is what we report below.
        </p>

        <table>
            <thead>
                <tr>
                    <th></th>
                    <th>Training</th>
                    <th>Test (dev)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Overall accuracy</td>
                    <td>0.6972</td>
                    <td>0.6940</td>
                </tr>
                <tr>
                    <td>Mfc  accuracy</td>
                    <td>0.2180</td>
                    <td>0.1918</td>
                </tr>
            </tbody>
        </table>

        <p>Test data metrics:</p>
        <table>
            <thead>
                     <tr>
                        <th></th>
                        <th>Label 0 (NEUTRAL)</th>
                        <th>Label 1 (JOY)</th>
                        <th>Label 2 (SADNESS)</th>
                        <th>Label 3 (ANGER)</th>
                        <th>Label 4 (DISGUST)</th>
                        <th>Label 5 (SURPRISE)</th>
                    </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Precision</td>
                    <td>0.7170</td>
                    <td>0.7021</td>
                    <td>0.5612</td>
                    <td>0.7429</td>
                    <td><span class="placeholder">No data</span></td>
                    <td>0.7705</td>
                </tr>
                <tr>
                    <td>Recall</td>
                    <td>0.7103</td>
                    <td>0.7021</td>
                    <td>0.6180</td>
                    <td>0.7723</td>
                    <td><span class="placeholder">No data</span></td>
                    <td>0.6438</td>
                </tr>
                <tr>
                    <td>F1</td>
                    <td>0.7136</td>
                    <td>0.7021</td>
                    <td>0.5882</td>
                    <td>0.7573</td>
                    <td><span class="placeholder">No data</span></td>
                    <td>0.7015</td>
                </tr>
            </tbody>
        </table>

        <p>Also, here is a graph of the LSTM's mean cross-entropy loss over about 100,000 epochs
        of training. (The loss reported is the average loss during the final 10 steps of the epoch).</p>

        <img src="lstmloss.png">

        <h3>Plans for future models</h3>

        <p>When more training data are collected, we will explore a word-level LSTM with
        a trained word embedding layer. We will also try using an emotional
        lexicon presented by <a href="http://saifmohammad.com/">Saif Mohammad</a>.
        </p>

<!--
UnigramClassifier
UnigramClassifier.train: vocabulary size is 1070
Training data
    Frequencies: [ 0.21373863  0.21685017  0.21804691  0.21517472  0.          0.13618956]
    Count:  4178
    Acc:    0.6776
    Mfc:    0.2180
Label 0 (NEUTRAL)
    Prec:   0.6717
    Rec:    0.6417
    F1: 0.6564
Label 1 (JOY)
    Prec:   0.6456
    Rec:    0.6777
    F1: 0.6613
Label 2 (SADNESS)
    Prec:   0.6442
    Rec:    0.7256
    F1: 0.6825
Label 3 (ANGER)
    Prec:   0.7341
    Rec:    0.6941
    F1: 0.7136
Label 4 (DISGUST)
    [absent]
Label 5 (SURPRISE)
    Prec:   0.7209
    Rec:    0.6309
    F1: 0.6729
Test data
    Frequencies: [ 0.23060345  0.20258621  0.19181034  0.21767241  0.          0.15732759]
    Count:  464
    Acc:    0.3513
    Mfc:    0.1918
Label 0 (NEUTRAL)
    Prec:   0.3333
    Rec:    0.2804
    F1: 0.3046
Label 1 (JOY)
    Prec:   0.4057
    Rec:    0.4574
    F1: 0.4300
Label 2 (SADNESS)
    Prec:   0.3084
    Rec:    0.3708
    F1: 0.3367
Label 3 (ANGER)
    Prec:   0.3922
    Rec:    0.3960
    F1: 0.3941
Label 4 (DISGUST)
    [absent]
Label 5 (SURPRISE)
    Prec:   0.2881
    Rec:    0.2329
    F1: 0.2576

LstmClassifier
Restored model from ./ckpts/lstm-113973
Training data
    Frequencies: [ 0.21373863  0.21685017  0.21804691  0.21517472  0.          0.13618956]
    Count:  4178
    Acc:    0.6972
    Mfc:    0.2180
Label 0 (NEUTRAL)
    Prec:   0.6814
    Rec:    0.7234
    F1: 0.7018
Label 1 (JOY)
    Prec:   0.7353
    Rec:    0.6347
    F1: 0.6813
Label 2 (SADNESS)
    Prec:   0.6763
    Rec:    0.7223
    F1: 0.6985
Label 3 (ANGER)
    Prec:   0.7157
    Rec:    0.7419
    F1: 0.7286
Label 4 (DISGUST)
    [absent]
Label 5 (SURPRISE)
    Prec:   0.6759
    Rec:    0.6450
    F1: 0.6601
Restored model from ./ckpts/lstm-113973
Test data
    Frequencies: [ 0.23060345  0.20258621  0.19181034  0.21767241  0.          0.15732759]
    Count:  464
    Acc:    0.6940
    Mfc:    0.1918
Label 0 (NEUTRAL)
    Prec:   0.7170
    Rec:    0.7103
    F1: 0.7136
Label 1 (JOY)
    Prec:   0.7021
    Rec:    0.7021
    F1: 0.7021
Label 2 (SADNESS)
    Prec:   0.5612
    Rec:    0.6180
    F1: 0.5882
Label 3 (ANGER)
    Prec:   0.7429
    Rec:    0.7723
    F1: 0.7573
Label 4 (DISGUST)
    [absent]
Label 5 (SURPRISE)
    Prec:   0.7705
    Rec:    0.6438
    F1: 0.7015
-->

        <h2>Text to Speech</h2>
        <p>We have made the following improvements to TTS since the previous baseline:
        </p>
        <ul>
            <li><strong>Multi-Class Pitch Envelopes</strong>: Additional pitch envelopes for the newly-supported classes.</li>
            <li><strong>Integration with Classifier</strong>: <code>text_to_speech.py</code> now provides a RESL (Read-Emote-Say-Loop) to quickly classify+speak text.</li>
            <li><strong>Integration with Front-End</strong>: We've set up a local Flask server with a web front-end to enable TTS from an attractive web interface (available on our <a href="https://github.com/team-butterfly/nlpcapstonecode">repo</a>):</li>
            <img src="demo.png" style="border-radius:8px; box-shadow: 0px 8px 32px -16px black, 0px 0px 0px 0.5px rgba(0,0,0,.1);"/>
        </ul>
        <p>In addition, we've begun work on a more advanced speech processing pipeline using style-transfer from emotive voices.  Initial experiments with a patch-based system have yielded poor results, so we intend to transition to a neural-network based post-processor for future iterations.</p>

        <hr />
        <p>The code for our baseline implementations is available on <a href="https://github.com/team-butterfly/nlpcapstonecode">GitHub</a>.</p>
    </div>

    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
    <script src="/js/applykatex.js"></script>
</body>
</html>
