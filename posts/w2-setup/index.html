<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 2 : Tool Setup</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
</head>
<body>
    <div class="post">
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Tool Setup</h1>
        <p>The following is a list of some of the tools we've set up to be used for this capstone project:
        </p>
        <h2>TensorFlow</h2>
        <p>We've installed <a href="https://www.tensorflow.org/install/install_linux">the GPU version of TensorFlow</a> on Attu.</p>
        <p>We've also written <a href="https://github.com/madebyollin/nlpcapstone/blob/master/code/rnnembed.py">a (simple) LSTM language model</a> to test that our configuration is correct, and to familiarize ourselves with the basics of the API.  It tries to predict the next word given the previous 9 words of history.</p>
        <p>It was frustrating to get the model working because we ran out of disk quota on Attu while installing spaCy's 1GB of word embeddings. Also, training this basic RNN is excruciatingly slow on a machine without GPUs. We will definitely need to use GPUs in order to successfully train and tune more complex models.</p>
        <h2>spaCy</h2>
        <p>spaCy is a nice NLP library that makes it super convenient to tokenize sentences into lexemes and retrieve GloVe vector embeddings for each lexeme, which was useful for the LSTM language model posted above.  We likely will use other (non-spaCy) embeddings in our project, but this was still a useful exercise.</p>
        <h2>Keras</h2>
        <p>For easier development of simple models, we've also set up <a href="https://keras.io/">Keras</a>, a high-level python framework which runs atop TensorFlow (or Theano) and provides a simple functional interface for creating networks.</p>
        <p>Keras appears to be more user-friendly and quick-to-write than TensorFlow (although both are comparatively high-level frameworks).</p>
        <p>Compared to using SciPy, making a neural net in Tensorflow was pleasantly high-level and free of crazy gradient calculations. Compared to using Keras, it was painstakingly verbose and devoid of useful abstractions.  To achieve efficiency and more rapid exploration of model architecture, especially given our short timeframe, we would definitely consider using Keras for our project.</p> <h2>GCE</h2>
        <p>We've received approval for (Google's default GPU cap is 0) and set up <a href="https://cloud.google.com/gpu/">a single-GPU instance</a> on Google Compute Engine to support  model development and training.  Right now this is financed with leftover credit from CSE 332.</p>
        <h2>Misc</h2>
        <p>We learned how to use the "say" command for simple text-to-speech on macOS, which can be used until we get a more exciting text-to-speech engine working.</p>
    </div>
</body>
</html>
