<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 1</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
</head>
<body>
    <div class="post">
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Project Ideas</h1>
        <p>We are considering the following project ideas (listed in no particular order):</p>
        <ul>
            <li><strong>Question Answering with MSMarco</strong></li>
            <li><strong>Sentimental Speech Generation</strong></li>
            <li><strong>Shallow Discourse Parsing</strong></li>
        </ul>
        <p>We will work in <!--(-->start-up mode<!-- | research mode)-->.</p>
        <!--
        List the top 3 project ideas your team is the most excited about.
        Briefly outline the minimal viable action plan with stretch goals.
        -->
        <h2>Question Answering with MSMarco<!--still in progress--></h2>
        <h3>Action Plan</h3>
        <ul>
            <li> Implement and train a Pointer Network architecture as an
            attention mechanism as described by
            <a href="https://arxiv.org/pdf/1506.03134.pdf">Vinyals et al.</a>.
            </li>
            <li> Implement and train an RNN/LSTM architecture
            to generate answers using the output of the attention model.
            For example, <a hrev="https://arxiv.org/pdf/1608.07905.pdf">Wang &amp; Jiang</a>.
            </li>
            <li> Open the domain to new inputs: Generate new training
            questions by crawling forums such as as Reddit, Quora,
            StackExchange, etc. for questions, and retrieving answer sources from a search
            engine.
            </li>
            <li> Make a user-friendly interface for the question-answering
            system.
            </li>
        </ul>
        <h3>Stretch Goals</h3>
        <ul>
            <li> Take advantage of MSMarco's "query type" annotations by making
            separate models for each query type. For example, "numeric" queries
            can be handled differently than "description" queries.
            </li>
        </ul>

        <h2>Sentimental Speech Generation</h2>
        <h3>Action Plan</h3>
        <ul>
            <li><strong>Train a Multi-Class Sentiment Analyzer:</strong> This would be implemented using a vanilla LSTM unless we stumble upon a more exciting architecture, and would focus on classifying the emotions of {anger, disgust, fear, joy, sadness, surprise}.  We can train on tweets containing emoji that correspond to those emotions (but with emoji removed).  We would likely use pre-trained Twitter word embeddings like GloVe.</li>
            <li><strong>Get an existing TTS System Running:</strong> Something like <a href="https://github.com/marytts/marytts">MaryTTS</a> would work.  Alternately, if there is a reasonable implementation of WaveNet, Deep Voice, or another more experimental architecture, this would be preferable.</li>
            <li><strong>Build a system to post-process TTS output according to sentiment:</strong> Initially, this would be hardcoded and applied per-sentence (<a href="https://en.wikipedia.org/wiki/Emotional_prosody">lower pitch and a sharper attack for anger</a>, for example), but depending on the quality of speech-to-emotion datasets available we could train a more carefully parameterized model.</li>
        </ul>
        <h3>Stretch Goals</h3>
        <ul>
            <li><strong>More Nuanced Text Analysis:</strong> Sentence-level analysis of basic emotions is good, but more classes (i.e. whether the sentence is a question, an order, etc.) can provide more information to improve synthesis.  Furthermore, computing word-level scores rather than sentence-level scores might provide more subtle readings.</li>
            <li><strong>Integrate Sentiment Processing with TTS as Conditioning:</strong> If we can implement <a href="https://github.com/ibab/tensorflow-wavenet/issues/112">local conditioning for TF-WaveNet</a>, for example, then we can train the speech synthesis model with emotion directly.</li>
        </ul>

        <h2>Shallow Discourse Parsing<span class="tag">[1]</span></h2>
        <h3>Action Plan</h3>
        <p>Implement a multi-step parsing pipeline as outlined in <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S1351324912000307">Lin et al., 2012</a>: discourse connective classification, argument segmentation, sense classification. Each of these steps can be tackled in a number of ways. As a minimal viable action plan, we propose:
            <ul>
                <li><strong>Discourse Connective Classification:</strong> matching directly on strings of known connectives</li>
                <li><strong>Argument Segmentation:</strong> token-level sequence labeling using CRFs (<a href="http://www.aclweb.org/anthology/I11-1120">Ghosh et al., 2011</a>)</li>
                <li><strong>Sense Classification:</strong> using Brown cluster pairs and coreference patterns (<a href="http://www.anthology.aclweb.org/E/E14/E14-1068.pdf">Rutherford &amp; Xue, 2014</a>)</li>
            </ul>
        </p>
        <h3>Stretch Goals</h3>
        <p>Optimized/more sophisticated models for the steps described above:
            <ul>
                <li>Syntactic features for connective classification (<a href="http://www.aclweb.org/anthology/P09-2004">Pitler &amp; Nenkova, 2009</a>)</li>
                <li>Latent variable RNN for implicit sense classification (<a href="https://arxiv.org/pdf/1603.01913.pdf">Ji et al., 2016</a>)</li>
                <li>Specialized argument extractors as described in <a href="http://aclweb.org/anthology/K15-2002">Wang &amp; Lan, 2015</a></li>
            </ul>
        </p>
        <p class="footnote">
            <span class="tag">1</span> Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi Prasad, Christopher Bryant, and Attapol T Rutherford. 2015. <a href="http://aclweb.org/anthology/K15-2001">The CoNLL-2015 shared task on shallow discourse parsing</a>. In <em>Proceedings of the Conference on Natural Language Learning (CoNLL)</em>.
        </p>

        <p>Our project (and blog) is <a href="https://github.com/madebyollin/nlpcapstone/">here on GitHub</a>.</p>
    </div>
</body>
</html>
