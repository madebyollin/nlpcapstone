<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 7 : Advanced Model Attempt I (Continued)</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
        integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
        crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"
        integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1"
        crossorigin="anonymous">
    </script>
</head>
<body>
    <div class="post">
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Advanced Model Attempt I (Continued)</h1>
        <p>We present an advanced model that builds upon our <a href="/posts/w4/">strawman</a> <a href="/posts/w5/">models</a>.</p>

        <h2>Data Collection</h2>
        <p>
            We've continued with the data collection scheme described in our
            <a href="/posts/w6/">previous update</a>, more than doubling the
            size of our corpus over the last week. The additional data has
            yielded a slight (~1-2%) improvement in the accuracy of the
            word-level LSTM.
        </p>
        <p>
            For our next advanced model, we plan to look into:
            <ul>
                <li>
                    <strong>Normalization:</strong> experiment with some
                    of the normalization techniques described in
                    <a href="https://noisy-text.github.io/2015/pdf/WNUT11.pdf">Supranovich & Patsepnia</a>.
                </li>
                <li>
                    <strong>Filtering:</strong> examine our corpus and look for
                    ways to produce a cleaner separation of data between the
                    various emotions.
                </li>
            </ul>
        </p>

        <h2>Classifier</h2>
        <p>
        We've implemented an <strong>attention mechanism</strong> for our word-level LSTM, modeled after
        the approach in <a href="https://arxiv.org/pdf/1604.00077.pdf">Shen and Lee</a>.
        Implementing this kind of attention in TensorFlow turned out to be more of a headache
        than we imagined, due to the dynamically-sized nature of an LSTM's inputs and
        intermediate tensors. In addition, we encountered driver issues on Azure that prevented
        GPU usage for a few days. So, progress on this front has been slower than we would
        like, but now it will continue full-steam ahead now that the worst parts are over.
        </p>
        
        <p>
        Shen and Lee use an attention mechanism that takes just the final state of
        a forward-LSTM and uses this to calculate cosine similarity with input word embeddings.
        </p>

        <img src="attention.png">

        <p>Right now, we are following this exactly, and using a sigmoid normalizer to
        smooth the attention among the input sequence. This model is able to achieve
        <strong>60%</strong> validation accuracy, which is similar to the non-attentive LSTM.</p>

        <p>We'd like to do more experimentation and error analysis and comparing different techniques
        of attention. In particular, it would be interesting to see how this attention model
        can be tweaked, and how it compares to a simple linear model over averaged word embeddings.</p>

        <p>
        Since we have a bidirectional LSTM, we have some interesting options
        available that we will explore:
        </p>

        <ul>
            <li>Take an element-wise product or sum of the forward and backward final states</li>
            <li>Concatenate the forward and backward states, then add a fully-connected layer to project them to the same
            size as the word embeddings</li>
        </ul>

        <h2>Text to Speech</h2>
        <p>We've continued the development of our adversarial speech/style style transfer model for emotional text-to-speech, this time focusing on the core autoencoder (rather than then adversarial loss).</p>
        <p>The plan for this model is to decompose input speech into <strong>time</strong> and frequency components, along with corresponding encoders for each, as well as a single decoder which combines these encodings back into the original speech signal.</p>
        <p>We've started with a simple convolutional architecture, using pooling and strided convolutions to downsample along the time and frequency axes.  The model encodes the input signal as a stack of filters for each time step, and the sample signal as a stack of filters for each frequency band.  The number of filters is not sufficient to reconstruct the original image, in order to induce the model to develop a useful internal representation.</p>

        <p>The primary failure mode of this architecture is to learn how to <em>factorize</em> the input spectrogram, without capturing any of its visual characteristics.  For example, this result from our early model:</p>
        <img src="factorization.png" />
        <p>The intended approaches to tackling this problem are as follows:</p>
        <ul>
            <li><strong>Train on disjoint signal/style pairs</strong>: rather than providing the autoencoder with identical inputs for the content and style during training, we intend to select disjoint samples from the same speaker.  This should make factorization much less effective.</li>
            <li><strong>Adversarial loss</strong>: although ensuring convergence in GANs is difficult (as identified in the previous blog post), they offer a promising way to ensure that the autoencoder generates plausible samples (rather than an average of plausible samples) for this ultimately underspecified problem.</li>
            <li><strong>Variational constraints</strong>: we hope to apply variational-autoencoder constraints on the hidden represententation (forcing it to be unit mean and variance) in order to further discourage factorization.</li>
        </ul>
    </div>
    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
    <script src="/js/applykatex.js"></script>
</body>
</html>
