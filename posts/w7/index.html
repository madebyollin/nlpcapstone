<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 7 : Advanced Model Attempt I (Continued)</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
        integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
        crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"
        integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1"
        crossorigin="anonymous">
    </script>
</head>
<body>
    <div class="post">
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Advanced Model Attempt I (Continued)</h1>
        <p>We present an advanced model that builds upon our <a href="/posts/w4/">strawman</a> <a href="/posts/w5/">models</a>.</p>

        <h2>Data Collection</h2>
        <p>
            We've continued with the data collection scheme described in our
            <a href="/posts/w6/">previous update</a>, more than doubling the
            size of our corpus over the last week. The additional data has
            yielded a slight (~1-2%) improvement in the accuracy of the
            word-level LSTM.
        </p>
        <p>
            For our next advanced model, we plan to look into:
            <ul>
                <li>
                    <strong>Normalization:</strong> experiment with some
                    of the normalization techniques described in
                    <a href="https://noisy-text.github.io/2015/pdf/WNUT11.pdf">Supranovich & Patsepnia</a>.
                </li>
                <li>
                    <strong>Filtering:</strong> examine our corpus and look for
                    ways to produce a cleaner separation of data between the
                    various emotions.
                </li>
            </ul>
        </p>

        <h2>Classifier</h2>

        <h2>Text to Speech</h2>
        <p>We've continued the development of our adversarial speech/style style transfer model for emotional text-to-speech, this time focusing on the core autoencoder (rather than then adversarial loss).</p>
        <p>The plan for this model is to decompose input speech into <strong>time</strong> and frequency components, along with corresponding encoders for each, as well as a single decoder which combines these encodings back into the original speech signal.</p>
        <p>We've started with a simple convolutional architecture, using pooling and strided convolutions to downsample along the time and frequency axes.  The model encodes the input signal as a stack of filters for each time step, and the sample signal as a stack of filters for each frequency band.  The number of filters is not sufficient to reconstruct the original image, in order to induce the model to develop a useful internal representation.</p>

        <p>The primary failure mode of this architecture is to learn how to <em>factorize</em> the input spectrogram, without capturing any of its visual characteristics.  For example, this result from our early model:</p>
        <img src="factorization.png" />
        <p>The intended approaches to tackling this problem are as follows:</p>
        <ul>
            <li><strong>Train on disjoint signal/style pairs</strong>: rather than providing the autoencoder with identical inputs for the content and style during training, we intend to select disjoint samples from the same speaker.  This should make factorization much less effective.</li>
            <li><strong>Adversarial loss</strong>: although ensuring convergence in GANs is difficult (as identified in the previous blog post), they offer a promising way to ensure that the autoencoder generates plausible samples (rather than an average of plausible samples) for this ultimately underspecified problem.</li>
            <li><strong>Variational constraints</strong>: we hope to apply variational-autoencoder constraints on the hidden represententation (forcing it to be unit mean and variance) in order to further discourage factorization.</li>
        </ul>
    </div>
    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
    <script src="/js/applykatex.js"></script>
</body>
</html>
