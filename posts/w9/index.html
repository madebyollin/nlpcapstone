<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 9 : Advanced Model Attempt II (Continued)</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
        integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
        crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"
        integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1"
        crossorigin="anonymous">
    </script>
</head>
<body>
    <div class="post">
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Advanced Model Attempt II (Continued)</h1>
        <p>We present an advanced model that builds upon our <a href="/posts/w6/">previous</a> <a href="/posts/w7/">advanced model</a>.</p>

        <h2>Data Collection and Processing</h2>
        <p>
            Our corpus is now at 195,000 tweets!
        </p>
        <p>
            We discovered that NLTK provides implementations of several
            different tokenizing methods, and we tried all of them out.
        </p>
        <table>
            <thead>
                <tr>
                    <th>Tokenizer</th>
                    <th>Token count</th>
                    <th>Vocabulary size</th>
                    <th>OOV&nbsp;rate (wrt.&nbsp;GloVe)</th>
                    <th>Peak accuracy</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>nltk.tokenize.wordpunct</code></td>
                    <td>3567549</td>
                    <td>185187</td>
                    <td>70.70%</td>
                    <td>60.62%</td>
                </tr>
                <tr>
                    <td><code>nltk.tokenize.word</code></td>
                    <td>3456080</td>
                    <td>196209</td>
                    <td>72.19%</td>
                    <td>60.91%</td>
                </tr>
                <tr>
                    <td><code>nltk.tokenize.treebank.TreebankWordTokenizer</code></td>
                    <td>3410336</td>
                    <td>205096</td>
                    <td>73.69%</td>
                    <td>60.96%</td>
                </tr>
                <tr>
                    <td><code>nltk.tokenize.repp.ReppTokenizer</code></td>
                    <td>3192813</td>
                    <td>192933</td>
                    <td>72.65%</td>
                    <td>60.15%</td>
                </tr>
                <tr>
                    <td><code>nltk.tokenize.moses.MosesTokenizer</code></td>
                    <td>3463779</td>
                    <td>195091</td>
                    <td>69.81%</td>
                    <td>60.37%</td>
                </tr>
                <tr>
                    <td><code>nltk.tokenize.casual.TweetTokenizer</code></td>
                    <td>3192813</td>
                    <td>192933</td>
                    <td>72.65%</td>
                    <td>59.75%</td>
                </tr>
            </tbody>
        </table>

        <h2>Classifier</h2>

        <p>This week, we've experimented with attention on hidden states (instead of word embeddings),
        re-trained with more tweets and better tokenization,
        and cleaned up our classifier code to pay off technical debt.</p>

        <h3>Attention</h3>

        <p>Previously, our attention mechanism targeted the input word embeddings.
        We noticed that the model had trouble classifying sentences with modifiers
        and negations, e.g. "this is f***ing great" was very confidently an "angry" phrase
        solely because of the profanity. We realized that attention on input embeddings effectively
        flattens the sentence into a weighted bag of words, which might be a cause of this.
        We've changed the attention mechanism to target the LSTM's hidden states in the hope that this may
        allow the model to capture the history and compositional
        meaning of words.</p>
        
        <p>To do this, we give each time step a score by a parameterized matrix multiplication between the
        LSTM outputs at that time step with the LSTM outputs at the final time step.
        At every time step, we concatenate the forward and backward LSTM outputs into a doubly-long vector.
        Then, we normalize the scores and then "summarize" the entire sequence with a weighted average
        of the concatenated LSTM outputs. This summary goes in to a fully-connected layer
        and softmax to create classifications.</p>

        <p>The validation accuracy of this type of attention is roughly the same as the previous one.
        From inspecting some example inputs, it's not clear if changing the attention has helped or not:</p>

        <div id="attention-key"></div>
        <ul>
            <li><div class="attention" data-filepath="rainy1.json"></div></li>
            <li><div class="attention" data-filepath="rainy2.json"></div></li>
            <li><div class="attention" data-filepath="great1.json"></div></li>
            <li><div class="attention" data-filepath="great2.json"></div></li>
            <li><div class="attention" data-filepath="parcel.json"></div></li>
            <li><div class="attention" data-filepath="lunch.json"></div></li>
        </ul>

        <p>The attention mechanism is still easily deceived by negations and syntactic words that
        change the meaning but don't inherently carry emotional baggage.</p>

        <h3>Odds and Ends</h3>

        <p>Our end-game plan for the classifier is to run tests to convince ourselves that our advanced
        model actually is learning what we think it's learning. Specifically:</p>
        <ul>
            <li><strong>Compare it to a model that just takes an average of the input word embeddings, and
            feeds this into a dense layer.</strong> This could give us evidence that the LSTM
            is actually doing something, as opposed to making the embedding layer
            do all the work.</li>
            
            <li><strong>Compare it to human performance on the same task.</strong> The
            model's accuracy has been rather stagnant, sitting at roughly 60%,
            despite changing the tokenization method and attention mechanism.
            This led us to realize that we don't have a good idea of what
            "optimal" performance on this dataset might look like.</li>

            <li><strong>Perform a more thorough hyperparameter search.</strong>
            We arbitrarily chose an LSTM hidden state size of 200. We haven't messed with
            the Adam optimizer's learning rate or epsilon. It would be nice to see how
            changing these parameters affects the model.</li>
        </ul>

        <h2>Text to Speech</h2>
        
    </div>
    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
    <script src="/js/applykatex.js"></script>
    <script src="/js/attention.js"></script>
</body>
</html>
