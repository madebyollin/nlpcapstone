<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 4 : Strawman I</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
    <!-- KaTeX -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
          integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
          crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"
            integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1"
            crossorigin="anonymous">
    </script>
</head>
<body>
    <div class="post">
        <!-- A good proposal should sketch out both the minimal viable action plan as well as stretch goals. Clearly state your project objectives, proposed methodologies, available resources, and the evaluation plan. Don't forget to include literature survey!-->
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Strawman I</h1>
        <p>We present an initial, baseline approach for our task of emotional text-to-speech generation.</p>

        <h2>Data Collection</h2>
        <p>
            We wrote a simple script to consume status updates from the Twitter
            <a href="https://dev.twitter.com/streaming/overview">Streaming APIs</a>
            (via <a href="https://github.com/tweepy/tweepy">Tweepy</a>). The script
            filters status updates that satisfy some basic criteria:
            <ul>
                <li>Written in English (<a href="https://dev.twitter.com/streaming/overview/request-parameters#language">machine detected</a>)</li>
                <li>At least 50 characters long</li>
                <li>Contains exactly one emoji, which must be at the end of the status udpate</li>
            </ul>
        </p>
        <p>
            The collected status updates are labeled according to the terminating
            emoji. For this first strawman, we used only the following mappings:
            <ul>
                <li>Joy: üòÄ üòÅ üòÉ üòÑ üôÇ</li>
                <li>Anger: üò° üò† üí¢</li>
            </ul>
        </p>
        <p>
            For the most parts, this gives reasonable results. Some examples of
            tweets correctly tagged by our script are:
        </p>
        <h4>Joy</h4>
        <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">What a great day, sun is shining and I am smiling üòÄ</p>&mdash; Darkleich Files (@darkleich_files) <a href="https://twitter.com/darkleich_files/status/853749486681804803">April 16, 2017</a></blockquote>
        <h4>Anger</h4>
        <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Weekend just got even worse. I hate all of you. üò°</p>&mdash; RellyRell (@HateTheseFools) <a href="https://twitter.com/HateTheseFools/status/853722382598631426">April 16, 2017</a></blockquote>
        <p>
            As it turns out, however, the üôÇ emoji is often used sarcastically, and
            many of the captured status updates with that emoji are <em>not</em>,
            in fact, joyful, such as:
        </p>
        <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I sure hope the devil likes a pretty face, because that attitude of yours is sending you straight to hell. üôÇ</p>&mdash; Kaylee (@k_greenroydd) <a href="https://twitter.com/k_greenroydd/status/853747338585804800">April 16, 2017</a></blockquote>

        <h2>Multi-Class Classification</h2>
        <span class="placeholder">Work-in-progress</span>
        <p>As a baseline, we use a dirt-simple unigram classifier that we implemented
        in TensorFlow. It classifies tweets by treating each tweet as a bag of words.
        Here's how it works:</p>
        <ul>
            <li>
                We pick a fixed label set (a set of emotions): <span class="katex-math">\mathcal{Y}</span>.
                For this baseline, <span class="katex-math">\mathcal{Y}=\{\texttt{JOY}, \texttt{ANGER}\}</span>.
            </li>
            <li>
                We pick a fixed vocabulary, <span class="katex-math">\mathcal{V}</span>
                by counting all tokens in the training data and choosing only the tokens
                that occur at least <span class="katex-math">k</span> times.
                For our baseline, we've arbitrarily chosen the cutoff <span class="katex-math">k=5</span>.
                We also assign each token a unique index
                <span class="katex-math">i \in \{0 \ldots |\mathcal{V}|-1\}</span>.
            </li>
            <li>
                To feed sentences to the model, we first encode each sentence as a vector
                <span class="katex-math">x \in \mathcal{R}^{|\mathcal{V}|}</span>
                such that
                <span class="katex-math">x_i</span> is set to the count of token
                <span class="katex-math">i</span> in the sentence.
            </li>
            <li>
                We predict a label <span class="katex-math">\hat{y}</span> using the function
                <div class="katex-math"> F(x) = \text{softmax}(xW + b) </div>
                <div class="katex-math"> \hat{y}(x) = \text{argmax\ }F(x) </div>
            </li>
            <li>
                So, the model's parameters to be learned are just a weights matrix and
                a bias vector:
                <div class="katex-math">
                    W \in \mathcal{R}^{|\mathcal{V}|\times|\mathcal{Y}|},\quad
                    b \in \mathcal{R}^{|\mathcal{Y}|}
                </div>
                We learn these by gradient descent on the cross-entropy loss function.
            </li>
        </ul>
        <p>Here are some performance metrics of the baseline model using 900 training tweets
        and 100 test tweets:</p>
        <table>
            <thead>Training data</thead>
            <tr>
                <td>Mfc</td>
                <td>49.8%</td>
            </tr>
            <tr>
                <td>Accuracy</td>
                <td>94.0%</td>
            </tr>
        </table>
        <table>
            <thead>Test data</thead>
            <tr>
                <td>Mfc</td>
                <td>48.0%</td>
            </tr>
            <tr>
                <td>Accuracy</td>
                <td>63.0%</td>
            </tr>
        </table>
        <p>Some examples of correct test classifications:</p>
        <ul>
            <li>
            <blockquote>
                @ AuntiePegg Love this film ! Makes me laugh every time 
            </blockquote>
            <samp>JOY</samp>
            </li>
            <li>
                <blockquote>
                    When you 're trying to watch a movie but people wo n't stop talking . 
                </blockquote>
                <samp>ANGER</samp>
            </li>
            <li>
                <blockquote>
                    People using Facebook to video themselves committing murders ? What the hell ... . 
                </blockquote>
                <samp>ANGER</samp>
            </li>
            <li>
                <blockquote>
                    What is wrong with these crazy fucking bastards in the world
                </blockquote>
                <samp>ANGER</samp>
            </li>
        </ul>
        <p>Some examples of incorrect test classifications:</p>
        <ul>
            <li>
                <blockquote>
                    @ Bryan_L_Johnson Happy Easter from the UK Brian ! 
                </blockquote>
                <samp>ANGER</samp>
            </li>
            <li>
                <blockquote>
                    lmaoo literally everything in my life sucks soo much rn 
                </blockquote>
                <samp>JOY</samp> 
            </li>
        </ul>

        <h2>Text to Speech</h2>
        <p>For our baseline text-to-speech model, we use the macOS <code>say</code> command and simple pitch envelopes.</p>
        <p>Our envelope for "happy" emotions is:</p>
        <span class="placeholder">Envelope 1</span>
        <p>Our envelope for "sad" emotions is:</p>
        <span class="placeholder">Envelope 2</span>
        <p>Some example text-to-speech outputs are:</p>
        <span class="placeholder">Happy</span>
        <span class="placeholder">Sad</span>
        <p>Envelopes are applied to STFT spectrograms and reconverted using ISTFT</p>
        <hr />
        <p>The code for our baseline implementations is available on <a href="https://github.com/team-butterfly/nlpcapstonecode">GitHub</a>.</p>
    </div>

    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
    <script src="/js/applykatex.js"></script>
</body>
</html>
