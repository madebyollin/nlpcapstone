<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Week 4 : Strawman I</title>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/butterfly.png">
    <!-- KaTeX -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
          integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
          crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"
            integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1"
            crossorigin="anonymous">
    </script>
</head>
<body>
    <div class="post">
        <!-- A good proposal should sketch out both the minimal viable action plan as well as stretch goals. Clearly state your project objectives, proposed methodologies, available resources, and the evaluation plan. Don't forget to include literature survey!-->
        <a class="header" href="/"><span class="butterfly"></span></a>
        <h1>Strawman I</h1>
        <p>We present an initial, baseline approach for our task of emotional text-to-speech generation.</p>

        <h2>Data Collection</h2>
        <p>We have constructed a simple data pipeline for collecting emotion-tagged sentences from Twitter.</p>
        <p>Our data collection process works as follows:</p>
        <ol>
            <li><span class="placeholder">Step the first</span></li>
            <li><span class="placeholder">Step the second</span></li>
            <li><span class="placeholder">Step three, the usurper</span></li>
        </ol>
        <p>Some examples of (correctly tagged) Tweets are as follows:</p>
        <h4>Happy</h4>
        <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Sprite gen work by my awesome undergrad mentee (Nick Liao, not on twitter). This is what they meant by training pokemon, right? <a href="https://t.co/iDx93hhpMa">pic.twitter.com/iDx93hhpMa</a></p>&mdash; Matthew Guzdial (@MatthewGuz) <a href="https://twitter.com/MatthewGuz/status/852234817383149569">April 12, 2017</a></blockquote>
        <h4>Sadness</h4>
        <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I&#39;m done with BEGAN and I don&#39;t want to do GAN again :( <a href="https://t.co/4e2gQqoz4W">https://t.co/4e2gQqoz4W</a> <a href="https://t.co/zAoVnWL2HP">pic.twitter.com/zAoVnWL2HP</a></p>&mdash; Taehoon Kim (@carpedm20) <a href="https://twitter.com/carpedm20/status/850570697399074816">April 8, 2017</a></blockquote>
        <p>An example of a Tweet that gets classified incorrectly is:</p>
        <span class="placeholder">example</span>

        <h2>Multi-Class Classification</h2>
        <span class="placeholder">Work-in-progress</span>
        <p>As a baseline, we use a dirt-simple unigram classifier. It classifies
        tweets by treating each tweet as a bag of words. Here's how it works:</p>
        <ul>
            <li>
                We pick a fixed label set (a set of emotions): <span class="katex-math">\mathcal{V}</span>
            </li>
            <li>
                We pick a fixed vocabulary, <span class="katex-math">\mathcal{V}</span>
                by counting all tokens in the training data and choosing only the words
                that occur at least <span class="katex-math">k</span> times.
                (For our baseline, we've arbitrarily chosen <span class="katex-math">k=5</span>.)
            </li>
            <li>
                We make predictions using the function
                <div class="katex-math">F(x) = \text{softmax}(xW + b)</div>
                The model's parameters to be learned are just a weights matrix and
                a bias vector:
                <ul>
                    <li>
                        <span class="katex-math">W \in \mathcal{R}^{|\mathcal{V}|\times|\mathcal{L}|}</span>
                    </li>
                    <li>
                        <span class="katex-math"> b \in \mathcal{R}^{|\mathcal{L}|}</span>
                    </li>
                </ul>
                We learned these by gradient descent on the cross-entropy loss function.
            </li>
        </ul>
        <p>This baseline classifier achieves <span class="placeholder">metrics</span>.</p>
        <p>Some examples of correct classifications:</p>

        <p>Some examples of incorrect classifications:</p>

        <h2>Text to Speech</h2>
        <p>For our baseline text-to-speech model, we use the macOS <code>say</code> command and simple pitch envelopes.</p>
        <p>Our envelope for "happy" emotions is:</p>
        <span class="placeholder">Envelope 1</span>
        <p>Our envelope for "sad" emotions is:</p>
        <span class="placeholder">Envelope 2</span>
        <p>Some example text-to-speech outputs are:</p>
        <span class="placeholder">Happy</span>
        <span class="placeholder">Sad</span>
        <p>Envelopes are applied to STFT spectrograms and reconverted using ISTFT</p>
        <hr />
        <p>The code for our baseline implementations is available on <a href="https://github.com/team-butterfly/nlpcapstonecode">GitHub</a>.</p>
    </div>

    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
    <script src="/js/applykatex.js"></script>
</body>
</html>
